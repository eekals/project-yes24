{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5381eb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c946169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    {\"name\": \"ì²˜ì„¸ìˆ _ì‚¶ì˜ ìì„¸\", \"url\": \"https://www.yes24.com/product/category/display/001001026008\"},\n",
    "    {\"name\": \"ì¸ê°„ê´€ê³„\", \"url\": \"https://www.yes24.com/product/category/display/001001026009\"},\n",
    "    {\"name\": \"ì„±ê³µí•™_ê²½ë ¥ê´€ë¦¬\", \"url\": \"https://www.yes24.com/product/category/display/001001026002\"},\n",
    "    {\"name\": \"í™”ìˆ _í˜‘ìƒ_íšŒì˜ì§„í–‰\", \"url\": \"https://www.yes24.com/product/category/display/001001026004\"},\n",
    "    {\"name\": \"ê¸°íš_ì •ë³´_ì‹œê°„ê´€ë¦¬\", \"url\": \"https://www.yes24.com/product/category/display/001001026003\"},\n",
    "    {\"name\": \"ì°½ì¡°ì ì‚¬ê³ _ë‘ë‡Œê³„ë°œ\", \"url\": \"https://www.yes24.com/product/category/display/001001026010\"},\n",
    "    {\"name\": \"ì—¬ì„±ì„ìœ„í•œìê¸°ê³„ë°œ\", \"url\": \"https://www.yes24.com/product/category/display/001001026001\"},\n",
    "    {\"name\": \"ì·¨ì—…_ìœ ë§ì§ì—…\", \"url\": \"https://www.yes24.com/product/category/display/001001026005\"},\n",
    "    {\"name\": \"ì„±ê³µìŠ¤í† ë¦¬\", \"url\": \"https://www.yes24.com/product/category/display/001001026012\"},\n",
    "    {\"name\": \"ìœ í•™_ì´ë¯¼\", \"url\": \"https://www.yes24.com/product/category/display/001001026011\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4191470b",
   "metadata": {},
   "source": [
    "## 1. ë„ì„œ ë§í¬ ìˆ˜ì§‘í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fce8f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import csv\n",
    "import os\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "output_dir = \"yes24_links\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# âœ… HTML êµ¬ì¡° ê¸°ë°˜ í˜ì´ì§€ ì´ë™ í•¨ìˆ˜\n",
    "async def go_to_page(page, target_page):\n",
    "    try:\n",
    "        print(f\"í˜ì´ì§€ {target_page} ì´ë™ ì‹œë„...\")\n",
    "        \n",
    "        # í˜„ì¬ í˜ì´ì§€ í™•ì¸\n",
    "        current_page_elem = await page.query_selector(\"div.yesUI_pagen strong.num\")\n",
    "        current_page = 1\n",
    "        if current_page_elem:\n",
    "            current_page = int(await current_page_elem.text_content())\n",
    "        \n",
    "        print(f\"í˜„ì¬ í˜ì´ì§€: {current_page}\")\n",
    "        \n",
    "        # ì´ë¯¸ ëª©í‘œ í˜ì´ì§€ì— ìˆìœ¼ë©´ ìŠ¤í‚µ\n",
    "        if current_page == target_page:\n",
    "            print(f\"âœ… ì´ë¯¸ í˜ì´ì§€ {target_page}ì— ìˆìŒ\")\n",
    "            return True\n",
    "        \n",
    "        # í˜„ì¬ í˜ì´ì§€ë„¤ì´ì…˜ ë¸”ë¡ ë²”ìœ„ í™•ì¸ (1-10, 11-20, 21-30, ...)\n",
    "        current_block_start = ((current_page - 1) // 10) * 10 + 1\n",
    "        current_block_end = current_block_start + 9\n",
    "        \n",
    "        target_block_start = ((target_page - 1) // 10) * 10 + 1\n",
    "        target_block_end = target_block_start + 9\n",
    "        \n",
    "        print(f\"í˜„ì¬ ë¸”ë¡: {current_block_start}-{current_block_end}, ëª©í‘œ ë¸”ë¡: {target_block_start}-{target_block_end}\")\n",
    "        \n",
    "        # ë‹¤ë¥¸ ë¸”ë¡ìœ¼ë¡œ ì´ë™í•´ì•¼ í•˜ëŠ” ê²½ìš°\n",
    "        if current_block_start != target_block_start:\n",
    "            if target_page > current_page:\n",
    "                # ì•ìœ¼ë¡œ ì´ë™: 'ë‹¤ìŒ' ë²„íŠ¼ í´ë¦­\n",
    "                clicks_needed = (target_block_start - current_block_start) // 10\n",
    "                for i in range(clicks_needed):\n",
    "                    next_button = await page.query_selector(\"a.bgYUI.next\")\n",
    "                    if next_button:\n",
    "                        print(f\"ğŸ”„ 'ë‹¤ìŒ' ë²„íŠ¼ í´ë¦­ ({i+1}/{clicks_needed})\")\n",
    "                        await next_button.click()\n",
    "                        await asyncio.sleep(2)\n",
    "                    else:\n",
    "                        print(f\"âŒ 'ë‹¤ìŒ' ë²„íŠ¼ ì—†ìŒ\")\n",
    "                        break\n",
    "            else:\n",
    "                # ë’¤ë¡œ ì´ë™: 'ì´ì „' ë²„íŠ¼ í´ë¦­ ë˜ëŠ” JavaScript ì§ì ‘ í˜¸ì¶œ\n",
    "                print(f\"ğŸ”§ JavaScriptë¡œ ì§ì ‘ í˜ì´ì§€ {target_page} ì´ë™\")\n",
    "                await page.evaluate(f'changeCategoryProductParam(null, {target_page});')\n",
    "                await page.wait_for_selector('div.itemUnit', state='visible', timeout=60000)\n",
    "                await asyncio.sleep(3)\n",
    "                return True\n",
    "        \n",
    "        # ê°™ì€ ë¸”ë¡ ë‚´ì—ì„œ í˜ì´ì§€ ì´ë™\n",
    "        page_anchor = await page.query_selector(f'div.yesUI_pagen a.num[title=\"{target_page}\"]')\n",
    "        if page_anchor:\n",
    "            print(f\"âœ… ë¸”ë¡ ë‚´ ë§í¬ í´ë¦­: í˜ì´ì§€ {target_page}\")\n",
    "            await page_anchor.click()\n",
    "        else:\n",
    "            # JavaScript ì§ì ‘ í˜¸ì¶œ fallback\n",
    "            print(f\"ğŸ”§ JavaScriptë¡œ í˜ì´ì§€ {target_page} ì´ë™\")\n",
    "            await page.evaluate(f'changeCategoryProductParam(null, {target_page});')\n",
    "        \n",
    "        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°\n",
    "        await page.wait_for_selector('div.itemUnit', state='visible', timeout=60000)\n",
    "        await asyncio.sleep(3)\n",
    "        \n",
    "        # ì´ë™ ê²°ê³¼ í™•ì¸\n",
    "        current_page_elem = await page.query_selector(\"div.yesUI_pagen strong.num\")\n",
    "        if current_page_elem:\n",
    "            final_page = int(await current_page_elem.text_content())\n",
    "            if final_page == target_page:\n",
    "                print(f\"âœ… í˜ì´ì§€ {target_page} ì´ë™ ì„±ê³µ\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"âš ï¸ í˜ì´ì§€ ì´ë™ ë¶ˆì¼ì¹˜: ëª©í‘œ {target_page}, ì‹¤ì œ {final_page}\")\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ í˜ì´ì§€ {target_page} ì´ë™ ì‹¤íŒ¨: {e}\")\n",
    "        return False\n",
    "\n",
    "# âœ… ì „ì²´ ë§í¬ ìˆ˜ì§‘ í•¨ìˆ˜ (ìˆœì„œ ìœ ì§€)\n",
    "async def collect_books_for_category(category):\n",
    "    collected_books = []\n",
    "    seen_books = set()\n",
    "    \n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=False)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(category[\"url\"])\n",
    "        await page.wait_for_timeout(3000)\n",
    "        \n",
    "        # íŒë§¤ëŸ‰ìˆœ í´ë¦­\n",
    "        try:\n",
    "            await page.click(\"a[data-search-value='SALE_SCO']\")\n",
    "            await page.wait_for_timeout(2000)\n",
    "        except:\n",
    "            print(\"âš ï¸ íŒë§¤ëŸ‰ìˆœ í´ë¦­ ì‹¤íŒ¨\")\n",
    "        \n",
    "        # 120ê°œ ë³´ê¸° ì„¤ì •\n",
    "        try:\n",
    "            await page.select_option(\"#pg_size\", value=\"120\")\n",
    "            await page.wait_for_timeout(3000)\n",
    "        except:\n",
    "            print(\"âš ï¸ 120ê°œ ë³´ê¸° ì„¤ì • ì‹¤íŒ¨\")\n",
    "        \n",
    "        # ì „ì²´ í˜ì´ì§€ ìˆ˜ íŒŒì•…ì„ ìœ„í•œ ìŠ¤í¬ë¡¤\n",
    "        print(\"ğŸ” ì „ì²´ í˜ì´ì§€ ìˆ˜ íŒŒì•… ì¤‘...\")\n",
    "        for _ in range(15):  # ë” ë§ì´ ìŠ¤í¬ë¡¤\n",
    "            await page.mouse.wheel(0, 3000)\n",
    "            await page.wait_for_timeout(1000)\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ í˜ì´ì§€ ì¶”ì¶œ (HTML êµ¬ì¡° ê¸°ë°˜)\n",
    "        last_page = 1\n",
    "        try:\n",
    "            # 'ë§¨ë' ë²„íŠ¼ì—ì„œ ì •í™•í•œ ë§ˆì§€ë§‰ í˜ì´ì§€ ì¶”ì¶œ\n",
    "            end_button = await page.query_selector(\"a.bgYUI.end\")\n",
    "            if end_button:\n",
    "                last_page = int(await end_button.get_attribute(\"title\"))\n",
    "                print(f\"ğŸ“„ ë§ˆì§€ë§‰ í˜ì´ì§€: {last_page}\")\n",
    "            else:\n",
    "                print(\"âš ï¸ 'ë§¨ë' ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\")\n",
    "                last_page = 10  # ê¸°ë³¸ê°’\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ë§ˆì§€ë§‰ í˜ì´ì§€ íŒŒì•… ì‹¤íŒ¨: {e}\")\n",
    "            last_page = 10  # ê¸°ë³¸ê°’\n",
    "        \n",
    "        print(f\"ğŸ” {category['name']} ì´ {last_page}í˜ì´ì§€ ìˆ˜ì§‘ ì‹œì‘\")\n",
    "        \n",
    "        # ê° í˜ì´ì§€ ìˆœíšŒ\n",
    "        for page_num in range(1, last_page + 1):\n",
    "            print(f\"\\n--- í˜ì´ì§€ {page_num}/{last_page} ---\")\n",
    "            \n",
    "            success = await go_to_page(page, page_num)\n",
    "            if not success:\n",
    "                print(f\"âŒ í˜ì´ì§€ {page_num} ê±´ë„ˆë›°ê¸°\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                book_nums = await page.eval_on_selector_all(\n",
    "                    \"#yesNewList > li[data-goods-no]\",\n",
    "                    \"els => els.map(el => el.getAttribute('data-goods-no'))\"\n",
    "                )\n",
    "                print(f\"ğŸ”— {len(book_nums)}ê°œ ë²ˆí˜¸ ìˆ˜ì§‘\")\n",
    "                \n",
    "                # ìˆœì„œë¥¼ ìœ ì§€í•˜ë©´ì„œ ì¤‘ë³µ ì œê±°\n",
    "                for book_num in book_nums:\n",
    "                    if book_num and book_num not in seen_books:\n",
    "                        collected_books.append(book_num)\n",
    "                        seen_books.add(book_num)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ í˜ì´ì§€ {page_num} ë²ˆí˜¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}\")\n",
    "            \n",
    "            # ì§„í–‰ë¥  í‘œì‹œ\n",
    "            if page_num % 10 == 0:\n",
    "                print(f\"ğŸ“Š ì§„í–‰ë¥ : {page_num}/{last_page} ({len(collected_books)}ê°œ ìˆ˜ì§‘)\")\n",
    "        \n",
    "        await browser.close()\n",
    "    \n",
    "    # CSV ì €ì¥\n",
    "    filename = os.path.join(output_dir, f\"{category['name']}_books.csv\")\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"book\"])\n",
    "        for book in collected_books:\n",
    "            writer.writerow([book])\n",
    "    \n",
    "    print(f\"âœ… ì €ì¥ ì™„ë£Œ: {filename} (ì´ {len(collected_books)}ê°œ)\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca7e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "await collect_books_for_category(categories[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca49faf",
   "metadata": {},
   "source": [
    "## 2. HTML ìˆ˜ì§‘í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96ec3de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# ê°œë³„ HTML ì €ì¥ í•¨ìˆ˜\n",
    "def fetch_and_save_html(book_id, category_dir, retries=3, delay=1):\n",
    "    url = f\"https://www.yes24.com/Product/Goods/{book_id}\"\n",
    "    save_path = os.path.join(category_dir, f\"{book_id}.html\")\n",
    "    \n",
    "    # ì´ë¯¸ ìˆ˜ì§‘ëœ ê²½ìš° ê±´ë„ˆëœ€\n",
    "    if os.path.exists(save_path):\n",
    "        return True\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "    }\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(response.text)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "# CSVë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³‘ë ¬ HTML ìˆ˜ì§‘\n",
    "def save_html_from_csv(csv_path, output_dir=\"htmls\", max_workers=20):\n",
    "    category_name = os.path.basename(csv_path).replace('_books.csv', '')\n",
    "    category_dir = os.path.join(output_dir, category_name)\n",
    "    os.makedirs(category_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    book_ids = df.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(fetch_and_save_html, book_id, category_dir): book_id for book_id in book_ids}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=f\"[{category_name}] HTML ìˆ˜ì§‘ ì¤‘\"):\n",
    "            book_id = futures[future]\n",
    "            if not future.result():\n",
    "                print(f\"âŒ ì‹¤íŒ¨: {book_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c17828e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ì¸ê°„ê´€ê³„] HTML ìˆ˜ì§‘ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4117/4117 [02:54<00:00, 23.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "save_html_from_csv(\"yes24_links/ì¸ê°„ê´€ê³„_books.csv\", output_dir=\"htmls\", max_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2168c6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ì²˜ì„¸ìˆ _ì‚¶ì˜ ìì„¸] HTML ìˆ˜ì§‘ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14678/14678 [10:16<00:00, 23.81it/s]\n"
     ]
    }
   ],
   "source": [
    "save_html_from_csv(\"yes24_links/ì²˜ì„¸ìˆ _ì‚¶ì˜ ìì„¸_books.csv\", output_dir=\"htmls\", max_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05435dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[í™”ìˆ _í˜‘ìƒ_íšŒì˜ì§„í–‰] HTML ìˆ˜ì§‘ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3773/3773 [02:34<00:00, 24.41it/s]\n"
     ]
    }
   ],
   "source": [
    "save_html_from_csv(\"yes24_links/í™”ìˆ _í˜‘ìƒ_íšŒì˜ì§„í–‰_books.csv\", output_dir=\"htmls\", max_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c74a89",
   "metadata": {},
   "source": [
    "## 3. HTML ì„ ëŒë©° ë„ì„œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e92dd4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b3631b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from bs4 import NavigableString\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def extract_book_info_from_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    title_elem = soup.select_one('h2.gd_name')\n",
    "    title = title_elem.text.strip() if title_elem else ''\n",
    "\n",
    "    pub_elem = soup.select_one('span.gd_pub a')\n",
    "    publisher = pub_elem.text.strip() if pub_elem else ''\n",
    "\n",
    "    pubdate_elem = soup.select_one('span.gd_date')\n",
    "    pub_date = pubdate_elem.text.replace('ì¶œê°„ì¼ :', '').strip() if pubdate_elem else ''\n",
    "\n",
    "    price_tag = soup.select_one('td span.nor_price em.yes_m')\n",
    "    price = price_tag.get_text(strip=True) if price_tag else None\n",
    "\n",
    "    discount = None\n",
    "    price_span = soup.select_one('span.nor_price')\n",
    "    if price_span and price_span.next_sibling:\n",
    "        sibling_text = price_span.next_sibling\n",
    "        if isinstance(sibling_text, NavigableString):\n",
    "            discount_match = re.search(r'(\\d+%)\\s*í• ì¸', sibling_text.strip())\n",
    "            if discount_match:\n",
    "                discount = discount_match.group(1)\n",
    "\n",
    "    point = None\n",
    "    li_tags = soup.select('li')\n",
    "    for li in li_tags:\n",
    "        text = li.get_text(strip=True)\n",
    "        if 'ì ë¦½' in text:\n",
    "            point = text\n",
    "            break\n",
    "\n",
    "    rating_tag = soup.select_one('span#spanGdRating em.yes_b')\n",
    "    rating = rating_tag.get_text(strip=True) if rating_tag else None\n",
    "\n",
    "    review_count_tag = soup.select_one('span.gd_reviewCount em.txC_blue')\n",
    "    review_count = review_count_tag.get_text(strip=True) if review_count_tag else None\n",
    "\n",
    "    sell_num = None\n",
    "    sell_num_tag = soup.select_one('span.gd_sellNum')\n",
    "    if sell_num_tag:\n",
    "        match = re.search(r'íŒë§¤ì§€ìˆ˜\\s*([\\d,]+)', sell_num_tag.get_text())\n",
    "        if match:\n",
    "            sell_num = match.group(1).replace(',', '')\n",
    "\n",
    "    table_of_contents = None\n",
    "    h4_tags = soup.select('h4.tit_txt')\n",
    "    for h4 in h4_tags:\n",
    "        if 'ëª©ì°¨' in h4.get_text(strip=True):\n",
    "            next_textarea = h4.find_next('textarea', class_='txtContentText')\n",
    "            if next_textarea:\n",
    "                table_of_contents = next_textarea.get_text(strip=True)\n",
    "            break\n",
    "\n",
    "    authors = []\n",
    "    author_groups = soup.select('div.authorInfoGrp')\n",
    "    if author_groups:\n",
    "        for group in author_groups:\n",
    "            author_id = group.get('authno', None)\n",
    "            role_elem = group.select_one('.author_name')\n",
    "            role_text = role_elem.text.strip() if role_elem else ''\n",
    "            role = role_text.split(':')[0].strip() if ':' in role_text else 'ì €ì'\n",
    "\n",
    "            for a in group.select('.author_name a.lnk_author'):\n",
    "                author_name = a.text.strip()\n",
    "                authors.append({\n",
    "                    'name': author_name,\n",
    "                    'role': role,\n",
    "                    'author_id': author_id\n",
    "                })\n",
    "    else:\n",
    "        auth_span = soup.select_one('span.gd_auth')\n",
    "        if auth_span:\n",
    "            # â‘  <a> íƒœê·¸ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì²˜ë¦¬\n",
    "            a_tags = auth_span.select('a')\n",
    "            if a_tags:\n",
    "                for a_tag in a_tags:\n",
    "                    author_name = a_tag.get_text(strip=True)\n",
    "                    role = ''\n",
    "                    next_sib = a_tag.next_sibling\n",
    "                    if next_sib and isinstance(next_sib, NavigableString):\n",
    "                        role = next_sib.strip()\n",
    "                    if not role:\n",
    "                        role = 'ì €ì'\n",
    "                    authors.append({'name': author_name, 'role': role, 'author_id': None})\n",
    "            else:\n",
    "                # â‘¡ <a> íƒœê·¸ ì—†ì´ í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš°\n",
    "                full_text = auth_span.get_text(strip=True)\n",
    "                # \"ê¹€ì°¬ë°° ì €\" í˜•ì‹ì´ë©´ ì—­í•  ë¶„ë¦¬\n",
    "                match = re.match(r'(.+?)\\s*(ì €|ì—­|ê·¸ë¦¼)?$', full_text)\n",
    "                if match:\n",
    "                    author_name = match.group(1).strip()\n",
    "                    role = match.group(2) or 'ì €ì'\n",
    "                    authors.append({'name': author_name, 'role': role, 'author_id': None})\n",
    "\n",
    "    return {\n",
    "        'title': title,\n",
    "        'publisher': publisher,\n",
    "        'pub_date': pub_date,\n",
    "        'price': price,\n",
    "        'discount': discount,\n",
    "        'point': point,\n",
    "        'rating': rating,\n",
    "        'review_count': review_count,\n",
    "        'sell_num': sell_num,\n",
    "        'table_of_contents': table_of_contents,\n",
    "        'authors': authors\n",
    "    }\n",
    "\n",
    "def parse_html_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        html = f.read()\n",
    "    return extract_book_info_from_html(html)\n",
    "\n",
    "def process_files_with_progress(file_list, max_workers=8):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(parse_html_file, f) for f in file_list]\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc='Parsing HTML files'):\n",
    "            try:\n",
    "                res = future.result()\n",
    "                results.append(res)\n",
    "            except Exception as e:\n",
    "                print(f'Error processing file: {e}')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f330a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fac0218d4464bd69f379565fa02ae48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing HTML files:   0%|          | 0/14678 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‘ì—… ì™„ë£Œ! csv íŒŒì¼ ì €ì¥ë¨.\n"
     ]
    }
   ],
   "source": [
    "file_list = glob.glob('htmls//*.html', recursive=True)\n",
    "results = process_files_with_progress(file_list, max_workers=5)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('yes24/ì²˜ì„¸ìˆ _ì‚¶ì˜ìì„¸.csv', index=False)\n",
    "\n",
    "print('ì‘ì—… ì™„ë£Œ! csv íŒŒì¼ ì €ì¥ë¨.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bc7c21",
   "metadata": {},
   "source": [
    "## 4. ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ec0faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder_path = \"./yes24\"  # yes24 í´ë” ê²½ë¡œ\n",
    "\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith(\".csv\")]\n",
    "\n",
    "# íŒŒì¼ì„ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ì´ì–´ë¶™ì´ê¸°\n",
    "merged_df = pd.concat(\n",
    "    [pd.read_csv(os.path.join(folder_path, file)) for file in csv_files],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(f\"ì´ í–‰ ê°œìˆ˜: {len(merged_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3a0519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# 1. í• ì¸ìœ¨: NaN â†’ '0%', % ì œê±° í›„ intí˜•\n",
    "merged_df['discount'] = merged_df['discount'].fillna('0%').astype(str).str.replace('%', '').astype(int)\n",
    "\n",
    "# 2. ì ë¦½ê¸ˆ: ì ë¦½ë¥ ë§Œ ìˆ«ì ì¶”ì¶œ, '5ë§Œì›ì´ìƒ êµ¬ë§¤ ì‹œ' ë“±ì€ 0 ì²˜ë¦¬\n",
    "import re\n",
    "def extract_point_percent(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    if '5ë§Œì›ì´ìƒ êµ¬ë§¤ ì‹œ' in text:\n",
    "        return 0\n",
    "    # ìˆ«ìì™€ % ê·¸ë¦¬ê³  ì ë¦½ ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ë¶€ë¶„ ì°¾ê¸°\n",
    "    match = re.search(r'\\((\\d+)%.*ì ë¦½\\)', text)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return 0\n",
    "\n",
    "merged_df['point'] = merged_df['point'].apply(extract_point_percent)\n",
    "\n",
    "# 3. íŒë§¤ê°€: NaN -> 0, ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ ìˆ«ì ì™¸ ë¬¸ì ì œê±°, ë¹ˆ ë¬¸ìì—´ 0 ì²˜ë¦¬ í›„ int ë³€í™˜\n",
    "merged_df['price'] = merged_df['price'].fillna(0).astype(str)\n",
    "merged_df['price'] = merged_df['price'].str.replace('[^0-9]', '', regex=True)\n",
    "merged_df.loc[merged_df['price'] == '', 'price'] = '0'\n",
    "merged_df['price'] = merged_df['price'].astype(int)\n",
    "\n",
    "# 4. í‰ì : NaN â†’ 0, floatí˜• ë³€í™˜\n",
    "merged_df['rating'] = merged_df['rating'].fillna(0).astype(float)\n",
    "\n",
    "# 5. ë¦¬ë·° ìˆ˜: NaN â†’ 0, intí˜• ë³€í™˜\n",
    "merged_df['review_count'] = (\n",
    "    merged_df['review_count']\n",
    "    .fillna('0')\n",
    "    .astype(str)\n",
    "    .str.replace(',', '', regex=True)\n",
    "    .astype(float)    # float ë³€í™˜\n",
    "    .astype(int)      # int ë³€í™˜\n",
    ")\n",
    "# 6. íŒë§¤ì§€ìˆ˜: NaN â†’ 0, intí˜• ë³€í™˜\n",
    "merged_df['sell_num'] = merged_df['sell_num'].fillna(0).astype(int)\n",
    "\n",
    "# 7. ë°œí–‰ì¼: datetime ë³€í™˜, ë³€í™˜ ì‹¤íŒ¨ ì‹œ NaT (ê²°ì¸¡ ìœ ì§€)\n",
    "merged_df['pub_date'] = pd.to_datetime(merged_df['pub_date'], format=\"%Yë…„ %mì›” %dì¼\", errors='coerce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6733953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‘ê°€ ì •ë³´ ì „ì²˜ë¦¬\n",
    "# 1. \"[{'name': 'ë¦¬ì—ì¦ˆ ì¹´ë€ ë“±ì € / ë°•ì˜ì¢…', 'role': 'ì—­', 'author_id': None}] ì²˜ëŸ¼ ë“¤ì–´ê°„ ë°ì´í„°ê°€ ì¡´ì¬\n",
    "import re\n",
    "import ast\n",
    "\n",
    "def parse_author_name_role(name_str, default_role):\n",
    "    # '/' ë¡œ ë¶„ë¦¬\n",
    "    parts = [p.strip() for p in name_str.split('/')]\n",
    "\n",
    "    authors_list = []\n",
    "    role_keywords = ['ì €', 'ë“±ì €', 'í¸', 'ì—­', 'ê·¸ë¦¼']  # í•„ìš”í•œ ì—­í•  í‚¤ì›Œë“œ ì¶”ê°€ ê°€ëŠ¥\n",
    "\n",
    "    for part in parts:\n",
    "        # ì—­í•  í‚¤ì›Œë“œ ì°¾ê¸°\n",
    "        found_role = None\n",
    "        for kw in role_keywords:\n",
    "            if part.endswith(kw):\n",
    "                found_role = kw\n",
    "                # ì´ë¦„ì€ ì—­í•  í‚¤ì›Œë“œ ëº€ ë¶€ë¶„\n",
    "                author_name = part[:-len(kw)].strip()\n",
    "                break\n",
    "        else:\n",
    "            # ì—­í•  í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ê¸°ë³¸ ì—­í•  í• ë‹¹\n",
    "            found_role = default_role\n",
    "            author_name = part\n",
    "        \n",
    "        authors_list.append({\n",
    "            'name': author_name,\n",
    "            'role': found_role,\n",
    "            'author_id': None\n",
    "        })\n",
    "\n",
    "    return authors_list\n",
    "\n",
    "\n",
    "def split_authors(authors_str):\n",
    "    try:\n",
    "        authors = ast.literal_eval(authors_str)\n",
    "    except Exception:\n",
    "        return authors_str\n",
    "\n",
    "    new_authors = []\n",
    "    for author in authors:\n",
    "        name = author.get('name', '')\n",
    "        role = author.get('role', '')\n",
    "\n",
    "        if '/' in name:\n",
    "            new_authors.extend(parse_author_name_role(name, role))\n",
    "        else:\n",
    "            new_authors.append(author)\n",
    "    return new_authors\n",
    "\n",
    "# ì ìš©\n",
    "merged_df['authors'] = merged_df['authors'].apply(split_authors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# authors ì—ì„œ name ì´ null ê°’ì¸ ê²½ìš°\n",
    "def has_null_name(authors):\n",
    "    if not isinstance(authors, list):\n",
    "        return False\n",
    "    for author in authors:\n",
    "        if author.get('name') in [None, '', ' ']:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "null_name_rows = merged_df[merged_df['authors'].apply(has_null_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a3e4f0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publisher</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>price</th>\n",
       "      <th>discount</th>\n",
       "      <th>point</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_count</th>\n",
       "      <th>sell_num</th>\n",
       "      <th>table_of_contents</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23501</th>\n",
       "      <td>ìƒìƒ ê³µë¶€ë¹„ë²•</td>\n",
       "      <td>ê°€ë¦¼ì¶œíŒì‚¬</td>\n",
       "      <td>2003-08-25</td>\n",
       "      <td>8550</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Part 1. ë‚˜ì˜ í•™ì°½ ì‹œì ˆì´ˆë“±í•™êµ | ì¤‘í•™êµ | ê³ ë“±í•™êµ, ì¬ìˆ˜ ê·¸ë¦¬ê³  ì‚¼ìˆ˜ | ë¯¸êµ­ ìœ í•™ ìƒí™œPart 2. ê³µë¶€ ì‹œì‘í•˜ê¸°ê³µë¶€ë€ ê²Œì„ê³¼ë„ ê°™ë‹¤ | ê³µë¶€ë¥¼ í•˜ëŠ” ì´ìœ  | ê³µë¶€ë¥¼ í•˜ê¸° ìœ„í•œ ë§ˆìŒê°€ì§Part 3. ìš°ë“±ìƒì´ ë˜ëŠ” ì‹­ê³„ëª…ì‹œê°„ì„ ì§€ë°°í•˜ë¼ | ê³„íšì„ ì„¸ì›Œë¼ | ìì‹ ë§Œì˜ ëª©í‘œë¥¼ ì„¸ì›Œë¼ |ìì‹ ì„ ì»¨íŠ¸ë¡¤í•˜ë¼ | í•œ ê³¼ëª©ì„ ìµœì†Œí•œ 1ì‹œê°„ ì´ìƒ ê³µë¶€í•˜ë¼ |ìŠ¤í„°ë”” ê·¸ë£¹ì„ ë§Œë“¤ì–´ë¼ | ì ì„ ì¶©ë¶„íˆ ìë¼ | ë†€ ë•Œ ë†€ê³  ê³µë¶€í•  ë•Œ ê³µë¶€í•˜ì |ì¶œì œì ì…ì¥ì—ì„œ ê³µë¶€í•˜ë¼ | í•  ìˆ˜ ìˆë‹¤ëŠ” ìì‹ ê°ì„ ê°–ìPart 4. ê³µë¶€ ê³ ìˆ˜ ë˜ê¸°ê³µë¶€ ê³ ìˆ˜ê°€ ë˜ê¸° ìœ„í•œ ìˆœì„œ | ê³µë¶€ ê³ ìˆ˜ê°€ ë˜ëŠ” ì§€ë¦„ê¸¸ | ì‹œí—˜ì„ ì˜ ë³´ëŠ” ë¹„ë²•Part 5. ê³µë¶€â€¦ ë³´ì´ì§€ ì•ŠëŠ” 1%ë¥¼ ìœ„í•´ì§‘ì¤‘ë ¥ì„ ê°–ì | ë…ì„œë¥¼ ë§ì´ í•˜ì | ê³µë¶€ì— ì‹œë„ˆì§€ íš¨ê³¼ë¥¼ ë°œíœ˜í•˜ì |ê³µë¶€ì— ëŒ€í•´ í”„ë¡œ ì˜ì‹ì„ ê°–ìPart 6. ìŠ¹ë¦¬ ìˆ˜í•™ ë”°ë¼ì¡ê¸°ì™œ ìˆ˜í•™ì„ ê³µë¶€í•´ì•¼ í•˜ëŠ”ê°€? | ì‚°ìˆ˜ì™€ ìˆ˜í•™ì˜ ê°œë… ì°¨ì´ |ìˆ˜í•™ì„ ì˜í•˜ë ¤ë©´ ì´ë ‡ê²Œ í•˜ì | ìˆ˜í•™, ë³´ì´ì§€ ì•ŠëŠ” 1%ë¥¼ ìœ„í•´ |ìˆ˜í•™ ì„ ìƒë‹˜ê»˜ ë“œë¦¬ëŠ” ë§ì”€Part 7. Jackieì˜ ì˜ì–´ ë”°ë¼ì¡ê¸°ì™œ ì˜ì–´ë¥¼ ë°°ì›Œì•¼ ë˜ì§€? | ì˜ì–´â€¦ ì–´ë–»ê²Œ ê³µë¶€í•˜ë¼ê³ !? | ìˆ˜ëŠ¥ ì˜ì–´ ì •ë³µí•˜ê¸° |ë¯¸êµ­ ìœ í•™ì„ ìƒê°í•˜ê³  ìˆëŠ” í•™ìƒë“¤ì—ê²ŒPart 8. ë“œë¦¬ê³  ì‹¶ì€ ë§ì”€í•™ë¶€ëª¨ë‹˜ê»˜ | ì„ ìƒë‹˜ê»˜ | í•™ìƒë“¤ì—ê²Œ |ë¯¸êµ­ì—ì„œ ë°”ë¼ë³´ëŠ” í•œêµ­ êµìœ¡ - ë¯¸êµ­ vs. í•œêµ­BEST Q&amp;A - í•™ìƒë“¤ì—ê²Œ ê°€ì¥ ë§ì´ ë°›ì•˜ë˜ ì§ˆë¬¸ë“¤ì ì€ ëª‡ ì‹œê°„ì”© ìì•¼ ë˜ë‚˜ìš”? | ìíˆ¬ë¦¬ ì‹œê°„ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•´ì•¼ ë˜ë‚˜ìš”? |ìŠ¬ëŸ¼í”„ëŠ” ì–´ë–»ê²Œ ê·¹ë³µí•´ì•¼ í•˜ë‚˜ìš”? | ì•”ê¸° ê³¼ëª©ì„ ì–´ë–»ê²Œ ê³µë¶€í•´ì•¼ í•˜ë‚˜ìš”? |ì‹œí—˜ì„ ë³´ê³  ë‚˜ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”? |í•™êµì—ì„œ ì–´ë–»ê²Œ ìƒí™œí•˜ê³  ê³µë¶€í•´ì•¼ í•˜ë‚˜ìš”? |ê³„íšëŒ€ë¡œ ì•ˆ ë  ë•ŒëŠ” ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?</td>\n",
       "      <td>[{'name': 'ì´ì€ìŠ¹', 'role': 'ì €', 'author_id': None}, {'name': '', 'role': 'ì €ì', 'author_id': None}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         title publisher   pub_date  price  discount  point  rating  \\\n",
       "23501  ìƒìƒ ê³µë¶€ë¹„ë²•     ê°€ë¦¼ì¶œíŒì‚¬ 2003-08-25   8550        10      5     0.0   \n",
       "\n",
       "       review_count  sell_num  \\\n",
       "23501             0         0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   table_of_contents  \\\n",
       "23501  Part 1. ë‚˜ì˜ í•™ì°½ ì‹œì ˆì´ˆë“±í•™êµ | ì¤‘í•™êµ | ê³ ë“±í•™êµ, ì¬ìˆ˜ ê·¸ë¦¬ê³  ì‚¼ìˆ˜ | ë¯¸êµ­ ìœ í•™ ìƒí™œPart 2. ê³µë¶€ ì‹œì‘í•˜ê¸°ê³µë¶€ë€ ê²Œì„ê³¼ë„ ê°™ë‹¤ | ê³µë¶€ë¥¼ í•˜ëŠ” ì´ìœ  | ê³µë¶€ë¥¼ í•˜ê¸° ìœ„í•œ ë§ˆìŒê°€ì§Part 3. ìš°ë“±ìƒì´ ë˜ëŠ” ì‹­ê³„ëª…ì‹œê°„ì„ ì§€ë°°í•˜ë¼ | ê³„íšì„ ì„¸ì›Œë¼ | ìì‹ ë§Œì˜ ëª©í‘œë¥¼ ì„¸ì›Œë¼ |ìì‹ ì„ ì»¨íŠ¸ë¡¤í•˜ë¼ | í•œ ê³¼ëª©ì„ ìµœì†Œí•œ 1ì‹œê°„ ì´ìƒ ê³µë¶€í•˜ë¼ |ìŠ¤í„°ë”” ê·¸ë£¹ì„ ë§Œë“¤ì–´ë¼ | ì ì„ ì¶©ë¶„íˆ ìë¼ | ë†€ ë•Œ ë†€ê³  ê³µë¶€í•  ë•Œ ê³µë¶€í•˜ì |ì¶œì œì ì…ì¥ì—ì„œ ê³µë¶€í•˜ë¼ | í•  ìˆ˜ ìˆë‹¤ëŠ” ìì‹ ê°ì„ ê°–ìPart 4. ê³µë¶€ ê³ ìˆ˜ ë˜ê¸°ê³µë¶€ ê³ ìˆ˜ê°€ ë˜ê¸° ìœ„í•œ ìˆœì„œ | ê³µë¶€ ê³ ìˆ˜ê°€ ë˜ëŠ” ì§€ë¦„ê¸¸ | ì‹œí—˜ì„ ì˜ ë³´ëŠ” ë¹„ë²•Part 5. ê³µë¶€â€¦ ë³´ì´ì§€ ì•ŠëŠ” 1%ë¥¼ ìœ„í•´ì§‘ì¤‘ë ¥ì„ ê°–ì | ë…ì„œë¥¼ ë§ì´ í•˜ì | ê³µë¶€ì— ì‹œë„ˆì§€ íš¨ê³¼ë¥¼ ë°œíœ˜í•˜ì |ê³µë¶€ì— ëŒ€í•´ í”„ë¡œ ì˜ì‹ì„ ê°–ìPart 6. ìŠ¹ë¦¬ ìˆ˜í•™ ë”°ë¼ì¡ê¸°ì™œ ìˆ˜í•™ì„ ê³µë¶€í•´ì•¼ í•˜ëŠ”ê°€? | ì‚°ìˆ˜ì™€ ìˆ˜í•™ì˜ ê°œë… ì°¨ì´ |ìˆ˜í•™ì„ ì˜í•˜ë ¤ë©´ ì´ë ‡ê²Œ í•˜ì | ìˆ˜í•™, ë³´ì´ì§€ ì•ŠëŠ” 1%ë¥¼ ìœ„í•´ |ìˆ˜í•™ ì„ ìƒë‹˜ê»˜ ë“œë¦¬ëŠ” ë§ì”€Part 7. Jackieì˜ ì˜ì–´ ë”°ë¼ì¡ê¸°ì™œ ì˜ì–´ë¥¼ ë°°ì›Œì•¼ ë˜ì§€? | ì˜ì–´â€¦ ì–´ë–»ê²Œ ê³µë¶€í•˜ë¼ê³ !? | ìˆ˜ëŠ¥ ì˜ì–´ ì •ë³µí•˜ê¸° |ë¯¸êµ­ ìœ í•™ì„ ìƒê°í•˜ê³  ìˆëŠ” í•™ìƒë“¤ì—ê²ŒPart 8. ë“œë¦¬ê³  ì‹¶ì€ ë§ì”€í•™ë¶€ëª¨ë‹˜ê»˜ | ì„ ìƒë‹˜ê»˜ | í•™ìƒë“¤ì—ê²Œ |ë¯¸êµ­ì—ì„œ ë°”ë¼ë³´ëŠ” í•œêµ­ êµìœ¡ - ë¯¸êµ­ vs. í•œêµ­BEST Q&A - í•™ìƒë“¤ì—ê²Œ ê°€ì¥ ë§ì´ ë°›ì•˜ë˜ ì§ˆë¬¸ë“¤ì ì€ ëª‡ ì‹œê°„ì”© ìì•¼ ë˜ë‚˜ìš”? | ìíˆ¬ë¦¬ ì‹œê°„ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•´ì•¼ ë˜ë‚˜ìš”? |ìŠ¬ëŸ¼í”„ëŠ” ì–´ë–»ê²Œ ê·¹ë³µí•´ì•¼ í•˜ë‚˜ìš”? | ì•”ê¸° ê³¼ëª©ì„ ì–´ë–»ê²Œ ê³µë¶€í•´ì•¼ í•˜ë‚˜ìš”? |ì‹œí—˜ì„ ë³´ê³  ë‚˜ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”? |í•™êµì—ì„œ ì–´ë–»ê²Œ ìƒí™œí•˜ê³  ê³µë¶€í•´ì•¼ í•˜ë‚˜ìš”? |ê³„íšëŒ€ë¡œ ì•ˆ ë  ë•ŒëŠ” ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?   \n",
       "\n",
       "                                                                                                authors  \n",
       "23501  [{'name': 'ì´ì€ìŠ¹', 'role': 'ì €', 'author_id': None}, {'name': '', 'role': 'ì €ì', 'author_id': None}]  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_name_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bd607777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§ì ‘ ìˆ˜ì •\n",
    "# 'ìƒìƒ ê³µë¶€ë¹„ë²•' ë„ì„œì˜ authors ì»¬ëŸ¼ ì§ì ‘ ìˆ˜ì •í•˜ê¸°\n",
    "idx = merged_df[merged_df['title'] == 'ìƒìƒ ê³µë¶€ë¹„ë²•'].index\n",
    "\n",
    "# ì˜ˆì‹œ: authorsë¥¼ ì˜¬ë°”ë¥´ê²Œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì§ì ‘ ì…ë ¥ (author_idëŠ” None ì²˜ë¦¬)\n",
    "merged_df.loc[idx, 'authors'] = [[{'name': 'ì´ì€ìŠ¹', 'role': 'ì €', 'author_id': None}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c652c37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ì €' 'ì—­' ',' 'ê³µ' 'í¸' 'í¸ì—­' 'ì €ì' 'ê³µì €' 'ì € /' 'ê°ìˆ˜' 'ê·¸ë¦¼' 'ì‚¬ì§„' 'ê¸°íš' 'ê°•ì˜' 'ì •ë¦¬'\n",
      " 'ê¸€' 'ê¸€ê·¸ë¦¼' 'ê³µì € /' 'ë“±ì €' 'í¸ì € /' 'ë“±ì—­' 'ê³µí¸' 'ì™¸ ê·¸ë¦¼' 'í¸ì €' 'í•´ì œ' 'ì›ì €' 'ê³µì—­' 'ë“±ì € /'\n",
      " 'ê·¸ë¦¼ /' 'í•´ì„¤' ', í¸ì§‘ë¶€ ê³µì €' 'ì™¸ ê°•ì˜' 'ì—®ìŒ' 'ì›ì „ /' 'ë¶í…”ëŸ¬' 'êµ¬ì—°' 'ì—­ /' 'ë“± ê°•ì˜' 'ê°ë…'\n",
      " 'ê¸€ /' 'ì™¸ ê³µì €' 'ì € / í¸ì§‘ë¶€ ì—­' 'ì—­,' 'í¸ /' 'ì¶œì—°' 'ê¸€, ê·¸ë¦¼' 'ì¼ëŸ¬ìŠ¤íŠ¸' 'ì—®ìŒ /' 'ì›ì‘ /'\n",
      " 'ë“±í¸' 'í‰ì—­' 'ì €,' 'í‰ì„¤' 'ê°ìˆ˜ /' 'ê¸°íš, í¸ì§‘' 'êµ¬ì„± /']\n"
     ]
    }
   ],
   "source": [
    "# role uniqueê°’í™•ì¸í•˜ê¸°\n",
    "# authors ë¦¬ìŠ¤íŠ¸ì—ì„œ ëª¨ë“  role ê°’ ëª¨ìœ¼ê¸°\n",
    "all_roles = merged_df['authors'].explode().dropna().apply(lambda x: x.get('role') if isinstance(x, dict) else None)\n",
    "\n",
    "# unique role ê°’ ì¶œë ¥\n",
    "unique_roles = all_roles.dropna().unique()\n",
    "print(unique_roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757bcee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì, ì—­ìë§Œ ê°€ì ¸ì˜¤ê¸°\n",
    "def filter_and_clean_authors(authors_list):\n",
    "    if not isinstance(authors_list, list):\n",
    "        return []\n",
    "    filtered = []\n",
    "    for author in authors_list:\n",
    "        role = author.get('role', '')\n",
    "        if not isinstance(role, str):\n",
    "            continue\n",
    "        if 'ì €' in role:\n",
    "            clean_role = 'ì €'\n",
    "        elif 'ì—­' in role:\n",
    "            clean_role = 'ì—­'\n",
    "        else:\n",
    "            continue\n",
    "        # ë³µì‚¬í•´ì„œ roleë§Œ ë°”ê¿”ì„œ ì¶”ê°€\n",
    "        new_author = author.copy()\n",
    "        new_author['role'] = clean_role\n",
    "        filtered.append(new_author)\n",
    "    return filtered\n",
    "\n",
    "merged_df['authors_filtered'] = merged_df['authors'].apply(filter_and_clean_authors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a6850be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns=['clean_roles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c794f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.rename(columns={\n",
    "    'title': 'ì œëª©',\n",
    "    'publisher': 'ì¶œíŒì‚¬',\n",
    "    'pub_date': 'ë°œí–‰ì¼',\n",
    "    'price': 'íŒë§¤ê°€',\n",
    "    'discount': 'í• ì¸ìœ¨',\n",
    "    'point': 'ì ë¦½ë¥ ',\n",
    "    'rating': 'í‰ì ',\n",
    "    'review_count': 'ë¦¬ë·°ìˆ˜',\n",
    "    'sell_num': 'íŒë§¤ì§€ìˆ˜',\n",
    "    'table_of_contents': 'ëª©ì°¨',\n",
    "    'authors': 'ì €ìì •ë³´',\n",
    "    'authors_filtered': 'ì €ì_ì—­ì_ì •ë¦¬'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "029d15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('yes24í†µí•©ë³¸.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
